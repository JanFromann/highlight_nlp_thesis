{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install fuzzywuzzy\n",
        "\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8mtdL3IiCO-",
        "outputId": "3e955b55-ac42-4367-ea6c-922b64bb0bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/73.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Get the data\n",
        "\n",
        "\n",
        "\n",
        "## FULL DATA\n",
        "\n",
        "# FINAL COMBINED DATA\n",
        "# Provide the correct path\n",
        "X_path = \"/content/drive/MyDrive/thesis_files/X_SBERT_FULLDATA_Final.pkl\"\n",
        "y_path = \"/content/drive/MyDrive/thesis_files/y_SBERT_FULLDATA_Final.pkl\"\n",
        "\n",
        "def split_data(X, y, train_size, val_size, test_size, random_state):\n",
        "    assert train_size + val_size + test_size == 1, \"train_size, val_size, and test_size must sum up to 1\"\n",
        "\n",
        "    # Split data into temporary and test datasets\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Adjust train_size to account for the previous split\n",
        "    train_size_adjusted = train_size / (train_size + val_size)\n",
        "\n",
        "    # Split the temporary dataset into train and validation datasets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=1 - train_size_adjusted, random_state=random_state\n",
        "    )\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Load 'X' data from pickle file\n",
        "with open(X_path, 'rb') as pkl_file:\n",
        "    X = pickle.load(pkl_file)\n",
        "\n",
        "# Load 'y' data from pickle file\n",
        "with open(y_path, 'rb') as pkl_file:\n",
        "    y = pickle.load(pkl_file)\n",
        "\n",
        "\n",
        "# Data and Device Management\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, train_size, val_size, test_size, random_state=42)\n"
      ],
      "metadata": {
        "id": "VuI7bcZY0cpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path where the files are stored\n",
        "path = \"/content/drive/MyDrive/thesis_files/\"\n",
        "\n",
        "# Load all_documents\n",
        "with open(path + 'all_documents.json', 'r') as f:\n",
        "    all_documents_final = json.load(f)\n",
        "\n",
        "# Load highlighted_documents\n",
        "with open(path + 'highlighted_documents.json', 'r') as f:\n",
        "    highlighted_documents_final = json.load(f)\n",
        "\n",
        "# Load gold_highlighted_documents\n",
        "with open(path + 'gold_highlighted_documents.json', 'r') as f:\n",
        "    gold_highlighted_documents_final = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "IrQ-obsaAlPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting up OpenAI API creds ###\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"key\"\n",
        "\n",
        "openai.organization = \"org\"\n",
        "\n",
        "openai_api_key = openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
        "\n",
        "assert openai_api_key is not None, \"OpenAI API key not found.\"\n",
        "openai.api_key = openai_api_key"
      ],
      "metadata": {
        "id": "thCd0k5ktg7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Prepare the DataFrame to collect results\n",
        "result_df = pd.DataFrame(columns=['Document', 'Highlighted_Sentences', 'Truncated'])\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "instruction_prompt = \"You are a model tasked to label sentences as highlights or normal text. Go through the full document above and return only the exact sentences that you think a human reader would highlight. Do not change sentences or hallucinate and only return full sentences.\"\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "instruct_count = len(encoding.encode(instruction_prompt))\n",
        "print(instruct_count)\n",
        "\n",
        "remaining_token_count = 4096 - instruct_count\n",
        "print(remaining_token_count)\n",
        "\n",
        "def analyze_document(document, instruction, instruct_count, out_count):\n",
        "\n",
        "    docs_token_count = 4096 - (instruct_count + out_count + 10)  #margin of safety, not sure why 7 tokens get added in the end\n",
        "\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Remove padding sentences and join into a single string\n",
        "    prompt_sentences = [sentence for sentence in document if sentence != 0]\n",
        "    prompt = \" \".join(prompt_sentences)\n",
        "\n",
        "    # Check if the prompt exceeds max_token_input and truncate if necessary\n",
        "    truncated = False\n",
        "    token_count = len(encoding.encode(prompt))\n",
        "\n",
        "    while token_count > docs_token_count:\n",
        "        truncated = True\n",
        "        prompt_sentences = prompt_sentences[:-1] # Remove the last sentence\n",
        "        prompt = \" \".join(prompt_sentences)\n",
        "        token_count = len(encoding.encode(prompt))\n",
        "\n",
        "\n",
        "    # Set truncated information\n",
        "    truncated_info = len(prompt_sentences) - 1 if truncated else False\n",
        "\n",
        "    full_prompt = f\"{prompt}\\n{instruction}\"\n",
        "\n",
        "    # Create the chat completion using the prompt and instruction\n",
        "    chat_completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        max_tokens=out_count\n",
        "    )\n",
        "\n",
        "    # Extract the highlighted sentences from the response\n",
        "    highlighted_sentences = chat_completion['choices'][0]['message']['content'].split('\\n')\n",
        "\n",
        "    return highlighted_sentences, truncated_info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1TGar2q8IFf",
        "outputId": "f2153fc1-c145-401e-beb9-6a74a20ecbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "4048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_text_within_quotes(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.count('\"') >= 2: # Check for at least two quotes\n",
        "            match = re.search(r'\"(.*?)\"', sentence)\n",
        "            cleaned_sentences.append(match.group(1))\n",
        "        else:\n",
        "            cleaned_sentences.append(sentence) # Keep the original sentence if there are not enough quotes\n",
        "    return cleaned_sentences\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zev6JJxVP48c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Ready Code ###\n",
        "\n",
        "\n",
        "from fuzzywuzzy import process\n",
        "import re\n",
        "import openai\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "instruction_prompt = \"You are a model tasked to label sentences as highlights or normal text. Go through the full document above and return only the exact sentences that you think a human reader would highlight. Do not change sentences or hallucinate and only return full sentences.\"\n",
        "\n",
        "\n",
        "def analyze_document(document, instruction, instruct_count, out_count):\n",
        "\n",
        "    docs_token_count = 4096 - (instruct_count + out_count + 10)  #margin of safety, not sure why 7 tokens get added in the end\n",
        "\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Remove padding sentences and join into a single string\n",
        "    prompt_sentences = [sentence for sentence in document if sentence != 0]\n",
        "    prompt = \" \".join(prompt_sentences)\n",
        "\n",
        "    # Check if the prompt exceeds max_token_input and truncate if necessary\n",
        "    truncated = False\n",
        "    token_count = len(encoding.encode(prompt))\n",
        "\n",
        "    while token_count > docs_token_count:\n",
        "        truncated = True\n",
        "        prompt_sentences = prompt_sentences[:-1] # Remove the last sentence\n",
        "        prompt = \" \".join(prompt_sentences)\n",
        "        token_count = len(encoding.encode(prompt))\n",
        "\n",
        "\n",
        "    # Set truncated information\n",
        "    truncated_info = len(prompt_sentences) - 1 if truncated else False\n",
        "\n",
        "    full_prompt = f\"{prompt}\\n{instruction}\"\n",
        "\n",
        "    # Create the chat completion using the prompt and instruction\n",
        "    chat_completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        max_tokens=out_count\n",
        "    )\n",
        "\n",
        "    # Extract the highlighted sentences from the response\n",
        "    highlighted_sentences = chat_completion['choices'][0]['message']['content'].split('\\n')\n",
        "\n",
        "    return highlighted_sentences, truncated_info\n",
        "\n",
        "\n",
        "def extract_text_within_quotes(sentences):\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Removing leading dash and spaces\n",
        "        sentence = sentence.strip().lstrip('-').strip()\n",
        "\n",
        "        if sentence.count('\"') >= 2: # Check for at least two quotes\n",
        "            match = re.search(r'\"(.*?)\"', sentence)\n",
        "            if match: # Ensure that a match was found\n",
        "                cleaned_sentences.append(match.group(1))\n",
        "            else:\n",
        "                cleaned_sentences.append(sentence) # Keep the original sentence if no quotes match\n",
        "        else:\n",
        "            cleaned_sentences.append(sentence) # Keep the original sentence if there are not enough quotes\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def verify_cleaned_sentences(cleaned_sentences, source_document):\n",
        "    matched_sentences = []\n",
        "\n",
        "    for cleaned_sentence in cleaned_sentences:\n",
        "        # Try a perfect match\n",
        "        if cleaned_sentence in source_document:\n",
        "            matched_sentences.append(cleaned_sentence)\n",
        "            continue\n",
        "\n",
        "        # Try a fuzzy match\n",
        "        best_match, score = process.extractOne(cleaned_sentence, source_document)\n",
        "        if score >= 80:\n",
        "            print(f\"Match found via fuzzy search: '{cleaned_sentence}'\")\n",
        "            matched_sentences.append(best_match)\n",
        "            continue\n",
        "\n",
        "        # Print error if no match found\n",
        "        print(f\"Error: The cleaned sentence could not be found in the source document: '{cleaned_sentence}'\")\n",
        "\n",
        "    return matched_sentences\n",
        "\n",
        "# Iterate over the dataset X\n",
        "\n",
        "def get_GPT_highlights(docs_data, instruction_prompt, output_count):\n",
        "\n",
        "    result_df = pd.DataFrame(columns=['Document_ID', 'GPT_Highlighted_Sentences', 'Truncated', 'Full_Document'])\n",
        "\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    instruct_count = len(encoding.encode(instruction_prompt))\n",
        "\n",
        "\n",
        "    for i, document in enumerate(docs_data):\n",
        "\n",
        "        highlighted_sentences =[]\n",
        "        cleaned_sentences = []\n",
        "        matched_sentences = []\n",
        "\n",
        "        highlighted_sentences, truncated_info = analyze_document(document, instruction=instruction_prompt, instruct_count=instruct_count, out_count=output_count)\n",
        "\n",
        "        cleaned_sentences = extract_text_within_quotes(highlighted_sentences)\n",
        "\n",
        "        matched_sentences = verify_cleaned_sentences(cleaned_sentences, document)\n",
        "\n",
        "        result_df.loc[i] = [i, matched_sentences, truncated_info, document]\n",
        "\n",
        "\n",
        "\n",
        "    # Save the DataFrame to a JSON file\n",
        "    result_df.to_json('GPT_highlighted_documents_test.json')\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "def check_sentences_in_document(df):\n",
        "    for index, row in df.iterrows():\n",
        "        document_id = row['Document_ID']\n",
        "        gpt_highlighted_sentences = row['GPT_Highlighted_Sentences']\n",
        "        full_document = row['Full_Document']\n",
        "\n",
        "        all_found = True\n",
        "        not_found_sentences = []\n",
        "\n",
        "        for sentence in gpt_highlighted_sentences:\n",
        "            if sentence not in full_document:\n",
        "                all_found = False\n",
        "                not_found_sentences.append(sentence)\n",
        "\n",
        "        print(f\"Document ID: {document_id}\")\n",
        "        print(f\"All sentences found: {all_found}\")\n",
        "        if not all_found:\n",
        "            print(f\"Sentences not found:\")\n",
        "            for sentence in not_found_sentences:\n",
        "                print(sentence)\n",
        "        print() # Blank line for separation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IQ8XOQV78J1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Run ###\n",
        "\n",
        "\n",
        "all_documents_final5, highlighted_documents_final5, gold_highlighted_documents_final5 = remove_first_n_sentences(all_documents_final, highlighted_documents_final, gold_highlighted_documents_final)\n"
      ],
      "metadata": {
        "id": "JObPKJbiHkdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_final = get_GPT_highlights(all_documents_final5, instruction_prompt, 200)"
      ],
      "metadata": {
        "id": "ABp0LbbuqFp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_final.to_json('results_final.json')\n"
      ],
      "metadata": {
        "id": "RhQpSRTqZvN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving results_final\n",
        "with open('results_final.json', 'w') as f:\n",
        "    json.dump(results_final, f)\n"
      ],
      "metadata": {
        "id": "E1FTKxF2NYGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('results_final.json')\n"
      ],
      "metadata": {
        "id": "njgxM8f0NZ3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_first_n_sentences(all_documents, highlighted_documents, gold_highlighted_documents, n=5):\n",
        "    def remove_from_highlighted_documents(original_doc, highlighted_doc):\n",
        "        removed_sentences = original_doc[:n]\n",
        "        return [sentence for sentence in highlighted_doc if sentence not in removed_sentences]\n",
        "\n",
        "    all_documents_modified = [doc[n:] for doc in all_documents]\n",
        "    highlighted_documents_modified = [remove_from_highlighted_documents(original_doc, highlighted_doc)\n",
        "                                      for original_doc, highlighted_doc in zip(all_documents, highlighted_documents)]\n",
        "    gold_highlighted_documents_modified = [remove_from_highlighted_documents(original_doc, gold_highlighted_doc)\n",
        "                                           for original_doc, gold_highlighted_doc in zip(all_documents, gold_highlighted_documents)]\n",
        "\n",
        "    return all_documents_modified, highlighted_documents_modified, gold_highlighted_documents_modified\n"
      ],
      "metadata": {
        "id": "w4_t5PTFgOxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_documents_final5_updated, highlighted_documents_final5_updated, gold_highlighted_documents_final5_updated = remove_first_n_sentences(all_documents_final, highlighted_documents_final, gold_highlighted_documents_final)\n"
      ],
      "metadata": {
        "id": "WLFRuPHjgZSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_final['Custom_Model_Highlights'] = highlighted_documents_final5_updated\n",
        "results_final['Gold_Highlights'] = gold_highlighted_documents_final5_updated\n",
        "results_final['Original_Full_Document'] = all_documents_final5"
      ],
      "metadata": {
        "id": "mS1nh5UtdGpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_final.drop(columns=['Custom_Model_Highlights', 'Gold_Highlights'], inplace=True)\n"
      ],
      "metadata": {
        "id": "9RyLlHtpdOzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_sentences_in_document(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xioEI5n2X68h",
        "outputId": "288e5212-f78e-4838-a7a1-47cf5d13ec47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document ID: 0\n",
            "All sentences found: True\n",
            "\n",
            "Document ID: 1\n",
            "All sentences found: True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_final.rename(columns={\n",
        "    'Custom_Model_Highlights': 'Custom Highlights',\n",
        "    'GPT_Highlighted_Sentences': 'Benchmark Highlights'\n",
        "}, inplace=True)\n"
      ],
      "metadata": {
        "id": "IlK5g7hthWn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_final.to_json('dataset_for_humaneva_final_0308_updated_naming.json', orient='records')"
      ],
      "metadata": {
        "id": "_8rdmkLyhSFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}