{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyA5xh5_7HXC",
        "outputId": "0bbd59e6-3d41-40b0-aa68-0d2f021602fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.32.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=993ac0b746802a19c5d47ba6b6616ec306ddad229ac7cf0a08fdbcfdcaf98c77\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=78b25c01e1cbb1d7d5f07c5d4a29b770ce45e0f25d2190804ea05277928b69d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers\n",
        "! pip install tqdm\n",
        "! pip install -U sentence-transformers\n",
        "! pip install wandb\n",
        "\n",
        "\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from html import escape\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from keras.layers import Masking\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import LSTM, Lambda\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import TimeDistributed, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input\n",
        "import wandb\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import io\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "import time\n",
        "from pathlib import Path\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from torch.nn.functional import softmax\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from torch.nn.functional import softmax\n",
        "import random\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from torch.nn.functional import softmax\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yU7cFx74xER",
        "outputId": "1c3f8bb6-52f4-4b8d-dc15-a93feb2d3d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHB79cAX7Nfb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyBDF8ZV7PAi"
      },
      "outputs": [],
      "source": [
        "## FULL DATA\n",
        "\n",
        "X_path = \"/content/drive/MyDrive/thesis_files/X_SBERT_FULLDATA_Final.pkl\"\n",
        "y_path = \"/content/drive/MyDrive/thesis_files/y_SBERT_FULLDATA_Final.pkl\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld59cBgX7Qm7"
      },
      "outputs": [],
      "source": [
        "## MEDIUM\n",
        "\n",
        "\n",
        "\n",
        "X_path = \"/content/drive/MyDrive/thesis_files/X_SBERT_medium_data_third_150.pkl\"\n",
        "y_path = \"/content/drive/MyDrive/thesis_files/y_SBERT_medium_data_third_150.pkl\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KgyJrIe7SZR"
      },
      "outputs": [],
      "source": [
        "# Load 'X' data from pickle file\n",
        "with open(X_path, 'rb') as pkl_file:\n",
        "    X = pickle.load(pkl_file)\n",
        "\n",
        "# Load 'y' data from pickle file\n",
        "with open(y_path, 'rb') as pkl_file:\n",
        "    y = pickle.load(pkl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHLH9GTO7UU7"
      },
      "outputs": [],
      "source": [
        "## CLASSES\n",
        "\n",
        "class HierarchicalSentenceBertForSentenceTagging(nn.Module):\n",
        "    def __init__(self, sbert_model_name, num_classes, dropout_rate=0.1, lstm_hidden_size=100, device=torch.device('cpu')):\n",
        "        super(HierarchicalSentenceBertForSentenceTagging, self).__init__()\n",
        "\n",
        "        self.sbert_model = SentenceTransformer(sbert_model_name)\n",
        "        self.lstm = nn.LSTM(self.sbert_model.get_sentence_embedding_dimension(), lstm_hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(2*lstm_hidden_size, num_classes)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, document_sentences):\n",
        "        # Get sentence embeddings for all sentences in all documents\n",
        "        sentence_embeddings = [self.sbert_model.encode(doc_sentences, convert_to_tensor=True) for doc_sentences in document_sentences]\n",
        "        sentence_embeddings = [torch.unsqueeze(se, 0).to(self.device) for se in sentence_embeddings]  # Unsqueeze and move each tensor to the device\n",
        "        sentence_embeddings = torch.cat(sentence_embeddings, 0)  # Concatenate along the new dimension\n",
        "\n",
        "        # Apply LSTM, Dropout, and Linear layer\n",
        "        lstm_outputs, _ = self.lstm(sentence_embeddings)\n",
        "        lstm_outputs = self.dropout(lstm_outputs)\n",
        "        logits = self.classifier(lstm_outputs)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Custom dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.data[index], self.labels[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2, ignore_index=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        if alpha is not None:\n",
        "            if isinstance(alpha, (list, tuple)):\n",
        "                self.alpha = torch.tensor(alpha).to(device)  # Move alpha to the device\n",
        "            elif isinstance(alpha, torch.Tensor):\n",
        "                self.alpha = alpha.to(device)  # Move alpha to the device\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        # Compute the negative log-likelihood\n",
        "        logpt = F.cross_entropy(input, target, reduction='none', ignore_index=self.ignore_index)\n",
        "        pt = torch.exp(-logpt)\n",
        "\n",
        "        # Compute the focal loss\n",
        "        if self.alpha is not None:\n",
        "            at = self.alpha.gather(0, target.data.view(-1))\n",
        "            logpt = logpt * at\n",
        "\n",
        "        focal_loss = (1-pt)**self.gamma * logpt\n",
        "\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujHhUZen7urn"
      },
      "outputs": [],
      "source": [
        "## FUNCTIONS\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def calculate_class_weights(y_train, device, class_h_factor=1.0):\n",
        "    # Concatenate all the labels\n",
        "    y_all = np.concatenate([doc_labels.cpu().numpy() for doc_labels in y_train])\n",
        "    #y_all = np.concatenate(y_train)\n",
        "\n",
        "    # Count unique labels\n",
        "    unique_labels, counts = np.unique(y_all, return_counts=True)\n",
        "\n",
        "    # Compute class weights and apply the class_h_factor to the second class\n",
        "    weights = 1.0 / counts\n",
        "    weights_dict = dict(zip(unique_labels, weights))\n",
        "\n",
        "    # Modify the weight of the highlight / 2 (third by zero index) class\n",
        "    if 1 in weights_dict:\n",
        "        weights_dict[2] *= class_h_factor\n",
        "\n",
        "    # Build weights list\n",
        "    weights_list = [weights_dict[i] if i in weights_dict else 0 for i in range(min(unique_labels), max(unique_labels)+1)]\n",
        "\n",
        "    # Convert to tensor\n",
        "    class_weights = torch.tensor(weights_list).float().to(device)\n",
        "\n",
        "    return class_weights, weights_dict\n",
        "\n",
        "def split_data(X, y, train_size, val_size, test_size, random_state):\n",
        "    assert train_size + val_size + test_size == 1, \"train_size, val_size, and test_size must sum up to 1\"\n",
        "\n",
        "    # Split data into temporary and test datasets\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Adjust train_size to account for the previous split\n",
        "    train_size_adjusted = train_size / (train_size + val_size)\n",
        "\n",
        "    # Split the temporary dataset into train and validation datasets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=1 - train_size_adjusted, random_state=random_state\n",
        "    )\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "def set_up_data(X, y, batch_size, collate_fn, train_size, val_size, test_size, random_state):\n",
        "    # Assuming split_data is a function that splits your data and returns X, y train, val, and test\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, train_size, val_size, test_size, random_state)\n",
        "    # Convert labels to tensors, convert to long integer type, and move to device\n",
        "    y_train = torch.tensor(y_train).long().to(device)\n",
        "    y_val = torch.tensor(y_val).long().to(device)\n",
        "    y_test = torch.tensor(y_test).long().to(device)\n",
        "\n",
        "    # Create instances of MyDataset for each of the splits\n",
        "    train_dataset = MyDataset(X_train, y_train)\n",
        "    val_dataset = MyDataset(X_val, y_val)\n",
        "    test_dataset = MyDataset(X_test, y_test)\n",
        "\n",
        "    # Create DataLoader instances for each of the datasets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    print(f'X_train len: {len(X_train)}')\n",
        "    print(f'X_val len: {len(X_val)}')\n",
        "    print(f'X_test len: {len(X_test)}')\n",
        "    # Return the data loaders\n",
        "    return train_dataloader, val_dataloader, test_dataloader, y_train\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, labels = zip(*batch)  # unzip the batch\n",
        "    sentences = list(sentences)\n",
        "\n",
        "    # Convert lists of labels to tensors. Before that, convert each list of labels to a tensor.\n",
        "    labels = [torch.tensor(l) for l in labels]\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=0)  # Assuming 0 will be ignored by the loss function.\n",
        "    return sentences, labels\n",
        "\n",
        "def setup_model(sbert_model_name, num_classes, device=None):\n",
        "    # Model creation\n",
        "    model = HierarchicalSentenceBertForSentenceTagging(sbert_model_name, num_classes, device=device)\n",
        "\n",
        "    # If a device is not provided, use CUDA if available\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Log model information to Weights and Biases\n",
        "    wandb.log({'model_name': sbert_model_name})\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def set_up_loss_optimizer(model, train_dataloader, num_epochs, learning_rate, eps, loss_type, use_scheduler=True, num_warmup_steps=0, class_weights=None, alpha=None, gamma=None):\n",
        "\n",
        "\n",
        "    # Only optimize parameters that require gradients (i.e., unfrozen parameters)\n",
        "    optimizer = AdamW([param for param in model.parameters() if param.requires_grad], lr=learning_rate, eps=eps)\n",
        "\n",
        "    if loss_type.lower() == 'focal':\n",
        "        # Use the provided alpha and gamma parameters for the Focal Loss\n",
        "        loss_function = FocalLoss(alpha=alpha, gamma=gamma, ignore_index=0)\n",
        "    elif loss_type.lower() == 'cross_entropy':\n",
        "        loss_function = nn.CrossEntropyLoss(weight=class_weights, ignore_index=0)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid loss_type. Expected 'focal' or 'cross_entropy'\")\n",
        "\n",
        "    if use_scheduler:\n",
        "        total_steps = len(train_dataloader) * num_epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    return optimizer, loss_function, scheduler\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, optimizer, loss_function, epoch, scheduler=None):\n",
        "    epoch += 1  # to not zero index\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_recall = 0\n",
        "    total_steps = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Train Epoch {epoch}')\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        sentences, labels = batch\n",
        "\n",
        "        outputs = model(sentences)  # Sentences remain as a list\n",
        "\n",
        "        batch_loss = 0\n",
        "        batch_predicted = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for doc_output, doc_label in zip(outputs, labels):\n",
        "            loss = loss_function(doc_output.view(-1, doc_output.shape[-1]), doc_label.view(-1))\n",
        "            batch_loss += loss  # Here, no .item()\n",
        "\n",
        "            _, predicted = torch.max(doc_output, 1)\n",
        "            batch_predicted.extend(predicted.view(-1).cpu().numpy())\n",
        "            batch_labels.extend(doc_label.view(-1).cpu().numpy())\n",
        "\n",
        "        batch_loss.backward()  # Here, the backward() call\n",
        "        optimizer.step()  # Here, the step() call\n",
        "\n",
        "        total_loss += batch_loss.item()  # Update total_loss after the optimizer step\n",
        "        total_steps += 1\n",
        "\n",
        "        # logging the loss?\n",
        "        wandb.log({\n",
        "                'epoch': epoch,\n",
        "                'Train/Loss': batch_loss / total_steps,\n",
        "                'Train/Accuracy': total_accuracy / total_steps,\n",
        "                'Train/Recall': total_recall / total_steps\n",
        "            })\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': batch_loss / total_steps,\n",
        "             'accuracy': total_accuracy / total_steps,\n",
        "            'recall': total_recall / total_steps\n",
        "            })\n",
        "        # Calculate accuracy and recall\n",
        "        total_accuracy += accuracy_score(batch_labels, batch_predicted)\n",
        "        total_recall += recall_score(batch_labels, batch_predicted, average='micro')\n",
        "\n",
        "\n",
        "\n",
        "    # Compute the average loss over the training data.\n",
        "    avg_train_loss = total_loss / total_steps\n",
        "    avg_train_accuracy = total_accuracy / total_steps\n",
        "    avg_train_recall = total_recall / total_steps\n",
        "\n",
        "    return avg_train_loss, avg_train_accuracy, avg_train_recall\n",
        "\n",
        "\n",
        "def validate_model(model, val_dataloader, loss_function, epoch):\n",
        "    epoch += 1\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_recall = 0\n",
        "    total_steps = 0\n",
        "\n",
        "    progress_bar = tqdm(val_dataloader, desc=f'Val Epoch {epoch}')\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            sentences, labels = batch\n",
        "            outputs = model(sentences)  # Sentences remain as a list\n",
        "\n",
        "            batch_loss = 0\n",
        "            batch_predicted = []\n",
        "            batch_labels = []\n",
        "\n",
        "            for doc_output, doc_label in zip(outputs, labels):\n",
        "                loss = loss_function(doc_output.view(-1, doc_output.shape[-1]), doc_label.view(-1))\n",
        "                batch_loss += loss #.item()\n",
        "\n",
        "                _, predicted = torch.max(doc_output, 1)\n",
        "                batch_predicted.extend(predicted.view(-1).cpu().numpy())\n",
        "                batch_labels.extend(doc_label.view(-1).cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy and recall\n",
        "            total_accuracy += accuracy_score(batch_labels, batch_predicted)\n",
        "            total_recall += recall_score(batch_labels, batch_predicted, average='micro')\n",
        "\n",
        "            total_steps += 1\n",
        "            total_loss += batch_loss.item()  # Update total_loss after all calculations\n",
        "\n",
        "\n",
        "            # Log metrics to wandb\n",
        "            wandb.log({\n",
        "                'epoch': epoch,\n",
        "                'Validation/Loss': batch_loss / total_steps,\n",
        "                'Validation/Accuracy': total_accuracy / total_steps,\n",
        "                'Validation/Recall': total_recall / total_steps\n",
        "            })\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': batch_loss / total_steps,\n",
        "                'accuracy': total_accuracy / total_steps,\n",
        "                'recall': total_recall / total_steps\n",
        "            })\n",
        "\n",
        "    # Compute the average loss over the validation data.\n",
        "    avg_val_loss = total_loss / total_steps\n",
        "    avg_val_accuracy = total_accuracy / total_steps\n",
        "    avg_val_recall = total_recall / total_steps\n",
        "\n",
        "    return avg_val_loss, avg_val_accuracy, avg_val_recall\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, test_dataloader):\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_steps = 0\n",
        "\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "\n",
        "    progress_bar = tqdm(test_dataloader, desc=f'Test')\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            sentences, labels = batch\n",
        "            outputs = model(sentences)  # Sentences remain as a list\n",
        "\n",
        "            for doc_output, doc_label in zip(outputs, labels):\n",
        "                doc_output_reshaped = doc_output.view(-1, doc_output.shape[-1])\n",
        "                doc_label_reshaped = doc_label.view(-1)\n",
        "                _, predicted = torch.max(doc_output, 1)\n",
        "                all_predicted.extend(predicted.view(-1).cpu().numpy())\n",
        "                all_labels.extend(doc_label.view(-1).cpu().numpy())\n",
        "\n",
        "            total_steps += 1\n",
        "\n",
        "        # Calculate accuracy and recall after all predictions are gathered\n",
        "        total_accuracy = accuracy_score(all_labels, all_predicted)\n",
        "        total_recall = recall_score(all_labels, all_predicted, average='micro')\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'accuracy': total_accuracy,\n",
        "            'recall': total_recall\n",
        "        })\n",
        "\n",
        "    # Calculate and print the classification report\n",
        "    classification_report_str = classification_report(all_labels, all_predicted, zero_division=1)\n",
        "    classification_report_dict = classification_report(all_labels, all_predicted, zero_division=1, output_dict=True)\n",
        "\n",
        "    # Log metrics and the classification report to wandb\n",
        "    wandb.log({\n",
        "        'Test/Accuracy': total_accuracy,\n",
        "        'Test/Recall': total_recall,\n",
        "        'Test/Classification Report': classification_report_dict\n",
        "    })\n",
        "\n",
        "    # Convert the classification report to a text plot\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.text(0.5, 0.5, classification_report_str, horizontalalignment='center', verticalalignment='center')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save the plot to a BytesIO object\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Load the image from the BytesIO object and log it to wandb\n",
        "    img = Image.open(buf)\n",
        "    wandb.log({'Test/Classification Report Image': [wandb.Image(img, caption='Classification Report')]})\n",
        "\n",
        "    # Close the BytesIO object\n",
        "    buf.close()\n",
        "\n",
        "    return total_accuracy, total_recall, all_labels, all_predicted\n",
        "\n",
        "\n",
        "def save_model_to_drive(model, model_name):\n",
        "    # Set the save path\n",
        "    save_path = \"/content/drive/MyDrive/thesis_files/SBERT/\" + model_name\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(f\"Model saved at {save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def inference_testing(json_file_path, model, name):\n",
        "    html_files = []\n",
        "    # Load data from json\n",
        "    data_df = pd.read_json(json_file_path)\n",
        "\n",
        "    # Get current date\n",
        "    date_str = datetime.date.today().strftime('%Y%m%d')\n",
        "\n",
        "    # Create a new directory for this run's HTML files\n",
        "    output_dir = f'/content/drive/MyDrive/thesis_files/SBERT/{name}_{date_str}'\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Iterate over documents in the DataFrame\n",
        "    for idx, row in data_df.iterrows():\n",
        "        title = row['title']\n",
        "        content = row['content']\n",
        "\n",
        "        # Run inference with the model\n",
        "        sentence_label_pairs = run_inference(model, content)\n",
        "\n",
        "        # Generate HTML document\n",
        "        html_content = html_highlight(sentence_label_pairs)\n",
        "\n",
        "        # Generate HTML file path\n",
        "        html_file_path = os.path.join(output_dir, f'{title}.html')\n",
        "\n",
        "        # Save HTML to a file\n",
        "        with open(html_file_path, 'w') as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        # Add HTML file path to the list\n",
        "        html_files.append(html_file_path)\n",
        "\n",
        "    return html_files\n",
        "\n",
        "\n",
        "def html_highlight(sentence_label_pairs, color=\"yellow\", sentences_per_paragraph=5):\n",
        "    \"\"\"\n",
        "    Generate HTML for the given sentences and labels.\n",
        "\n",
        "    Args:\n",
        "        sentence_label_pairs (List[Tuple[str, str]]): The list of pairs of sentences and labels.\n",
        "        color (str, optional): The highlight color for 'highlight' labels.\n",
        "            Defaults to \"yellow\".\n",
        "        sentences_per_paragraph (int, optional): The number of sentences per paragraph.\n",
        "            Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated HTML string.\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    sentence_count = 0\n",
        "    for sentence, label in sentence_label_pairs:\n",
        "        if label == 'highlight':\n",
        "            html.append(f'<mark style=\"background-color: {color};\">{escape(sentence)}</mark>')\n",
        "        else:\n",
        "            html.append(escape(sentence))\n",
        "\n",
        "        sentence_count += 1\n",
        "        if sentence_count == sentences_per_paragraph:\n",
        "            html.append('<br/><br/>')\n",
        "            sentence_count = 0\n",
        "\n",
        "    html_text = ' '.join(html)\n",
        "\n",
        "    # Add HTML and CSS to resemble Medium blog post style\n",
        "    html_text = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "    <style>\n",
        "    body {{\n",
        "        font-family: 'Times New Roman', Times, serif;\n",
        "        color: #444;\n",
        "        background-color: #FAFAFA;\n",
        "        margin: 0;\n",
        "        padding: 0;\n",
        "    }}\n",
        "    .content {{\n",
        "        max-width: 800px;\n",
        "        margin: 0 auto;\n",
        "        padding: 2em;\n",
        "        font-size: 18px;\n",
        "        line-height: 1.6;\n",
        "    }}\n",
        "    mark {{\n",
        "        background-color: #FFF9C4;\n",
        "        padding: 0.2em;\n",
        "    }}\n",
        "    </style>\n",
        "    </head>\n",
        "    <body>\n",
        "    <div class=\"content\">\n",
        "    {html_text}\n",
        "    </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return html_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_inference(model, document):\n",
        "\n",
        "    label_tokenizer = Tokenizer(filters='', lower=False)\n",
        "\n",
        "    # Explicitly set the word_index\n",
        "    label_tokenizer.word_index = {'normaltext': 1, 'highlight': 2}\n",
        "    # This assumes 'X' is used for subword tokens and 'PAD' is used for padding tokens\n",
        "    label_tokenizer.word_index['X'] = len(label_tokenizer.word_index) + 1\n",
        "    label_tokenizer.word_index['PAD'] = 0\n",
        "\n",
        "    # Tokenize document into sentences\n",
        "    sentences = sent_tokenize(document)\n",
        "\n",
        "    # Process sentences to match the model's expected input format\n",
        "    document_sentences = [sentences]  # Model expects a list of lists of sentences\n",
        "\n",
        "    # Run model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(document_sentences)\n",
        "        predictions = torch.argmax(output, dim=2)\n",
        "        probabilities = softmax(output, dim=2)\n",
        "\n",
        "    # Convert tensor of predictions to list\n",
        "    predictions = predictions.cpu().numpy().tolist()\n",
        "\n",
        "    # Convert label indices back to label strings\n",
        "    index_to_tag = {v: k for k, v in label_tokenizer.word_index.items()}\n",
        "    label_predictions = [[index_to_tag[pred] for pred in doc_preds] for doc_preds in predictions]\n",
        "\n",
        "    # Pair each sentence with its predicted label\n",
        "    sentence_label_pairs = [(sent, label) for sent, label in zip(sentences, label_predictions[0])]\n",
        "    print(sentence_label_pairs)\n",
        "    return sentence_label_pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "QcUqInFJ7TAj",
        "outputId": "72ccb917-8384-496f-de83-26aebdbdc67d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjf-frommann\u001b[0m (\u001b[33mbitdat-com\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230824_165137-hjmkd6bm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bitdat-com/project/runs/hjmkd6bm' target=\"_blank\">name</a></strong> to <a href='https://wandb.ai/bitdat-com/project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bitdat-com/project' target=\"_blank\">https://wandb.ai/bitdat-com/project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bitdat-com/project/runs/hjmkd6bm' target=\"_blank\">https://wandb.ai/bitdat-com/project/runs/hjmkd6bm</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## SETUP\n",
        "\n",
        "\n",
        "# Set up your Weights & Biases project\n",
        "run_name = \"name\"\n",
        "wandb.init(project=\"project\", name=run_name)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "wandb.log({'Device': str(device)})\n",
        "\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7E5CYM65ocy"
      },
      "outputs": [],
      "source": [
        "## RUN\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "wandb.config.seed = SEED\n",
        "\n",
        "# Data and Device Management\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "\n",
        "# Convert labels to tensors, convert to long integer type, and move to device\n",
        "\n",
        "\n",
        "# Set hyperparameters\n",
        "sbert_model_name = 'all-MiniLM-L6-v2'\n",
        "data_set_type='medium' # 'all' or 'medium'\n",
        "loss_type='focal' # 'focal' or 'cross_entropy'\n",
        "\n",
        "num_epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "learning_rate = 0.001\n",
        "eps = 1e-8\n",
        "\n",
        "num_classes = 3\n",
        "class_h_factor = 1\n",
        "\n",
        "min_alpha= 0.012\n",
        "max_alpha = 1- min_alpha\n",
        "print(max_alpha)\n",
        "focal_alpha=[0,min_alpha,max_alpha,0]\n",
        "focal_gamma=2.0\n",
        "\n",
        "use_scheduler=False\n",
        "\n",
        "# Log hyperparameters\n",
        "wandb.config.update({\n",
        "    \"learning_rate\" : learning_rate,\n",
        "    \"sbert_model_name\" : sbert_model_name,\n",
        "    \"eps\": eps,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"sbert_model_name\": sbert_model_name,\n",
        "    \"class_h_factor\" : class_h_factor,\n",
        "    \"use_scheduler\" : use_scheduler,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"data_set_type\" : data_set_type,\n",
        "    \"loss_type\": loss_type,\n",
        "    \"train_size\": train_size,\n",
        "    \"val_size\": val_size,\n",
        "    \"test_size\": test_size,\n",
        "    \"min_alpha\": min_alpha\n",
        "})\n",
        "\n",
        "# Set-up data loaders\n",
        "train_dataloader, val_dataloader, test_dataloader, y_train = set_up_data(X, y, batch_size, collate_fn, train_size, val_size, test_size, random_state=SEED)\n",
        "\n",
        "# Set-up the model\n",
        "model = setup_model(sbert_model_name, num_classes, device)\n",
        "\n",
        "# Calculate class weights for the loss function\n",
        "class_weights, _ = calculate_class_weights(y_train, device, class_h_factor)\n",
        "\n",
        "# Set-up loss function, optimizer, and scheduler\n",
        "optimizer, loss_function, scheduler = set_up_loss_optimizer(model, train_dataloader, num_epochs, learning_rate, eps, loss_type, use_scheduler=use_scheduler, num_warmup_steps=0, class_weights=class_weights, alpha=focal_alpha, gamma=focal_gamma)\n",
        "\n",
        "# Train and validate the model\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy, train_recall = train_model(model, train_dataloader, optimizer, loss_function, epoch, scheduler)\n",
        "    val_loss, val_accuracy, val_recall = validate_model(model, val_dataloader, loss_function, epoch)\n",
        "\n",
        "# Test the model\n",
        "test_accuracy, test_recall, all_labels, all_predicted = test_model(model, test_dataloader)\n",
        "\n",
        "# save the model\n",
        "model_name = run_name + '.pt'\n",
        "save_model_to_drive(model, model_name) # set-up\n",
        "\n",
        "html_files = inference_testing('/content/drive/MyDrive/thesis_files/inference_docs_file.json', model, run_name)\n",
        "\n",
        "\n",
        "# Upload each HTML file to Weights & Biases\n",
        "for file in html_files:\n",
        "    wandb.save(file)\n",
        "\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}