{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiociQvYmJxw"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "from keras.layers import Masking\n",
        "# Import required layers and modules\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDqfFEX8ZKJg",
        "outputId": "9813c843-ae24-4425-ab50-68d3cb3cad9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### loading the full dataset ###\n",
        "\n",
        "combined_df_wordlabelled = pd.read_json('/content/drive/MyDrive/thesis_files/combined_df_wordlabelled_1706.json', orient='index')"
      ],
      "metadata": {
        "id": "r3Loq3BmZ1ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### loading the medium set ###\n",
        "\n",
        "combined_df_wordlabelled = pd.read_json('/content/drive/MyDrive/thesis_files/medium_second_wordlabelled_0407.json', orient='index')\n",
        "\n"
      ],
      "metadata": {
        "id": "Z8Bh3WEZNqnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_documents(dataset, n):\n",
        "    new_dataset = []\n",
        "    for document in dataset:\n",
        "        for i in range(0, len(document), n):\n",
        "            new_document = document[i:i+n]\n",
        "            new_dataset.append(new_document)\n",
        "    return new_dataset"
      ],
      "metadata": {
        "id": "JG9dN0DCaJkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df_seqlabelled_word_short = split_documents(combined_df_wordlabelled['tagged_text'], 512)"
      ],
      "metadata": {
        "id": "1oHerZiIaLRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(combined_df_seqlabelled_word_short)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GGJciu2aPmF",
        "outputId": "f9138b11-3492-4ffa-fa91-91afa31f84ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99115"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(max(combined_df_seqlabelled_word_short, key=len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-LlQyZKaNSa",
        "outputId": "6fbfc2f0-3ce1-4686-ae8f-35949de9e6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = combined_df_seqlabelled_word_short"
      ],
      "metadata": {
        "id": "L3SHrU0kaqBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_tags(tagged_data):\n",
        "    renamed_data = []\n",
        "\n",
        "    for sequence in tagged_data:\n",
        "        new_sequence = []\n",
        "        for word, tag in sequence:\n",
        "            #print(word, tag)\n",
        "            if tag == 'non-highlight':\n",
        "                new_sequence.append((word, 'normaltext'))\n",
        "            else:\n",
        "                new_sequence.append((word, tag))\n",
        "        renamed_data.append(new_sequence)\n",
        "\n",
        "    return renamed_data"
      ],
      "metadata": {
        "id": "_7iM3X1N-B4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = rename_tags(tagged_data)"
      ],
      "metadata": {
        "id": "puLK_55B-FbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(seq) for seq in tagged_data])\n",
        "print(f\"Maximum sequence length: {max_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "092h9JhL-JDW",
        "outputId": "dd68a2f9-8910-4596-9654-5e9f32c7fafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = tagged_data"
      ],
      "metadata": {
        "id": "8ju8uDlVvicE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = tagged_data[:500]"
      ],
      "metadata": {
        "id": "d73Zfe6IvmHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = full_data"
      ],
      "metadata": {
        "id": "Z1tN48FxPmv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tagged_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FH_2VzFPpk0",
        "outputId": "3dc103b6-934f-45d8-dc6a-801692afe0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11551"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_data[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uiwdyeiO3Dv",
        "outputId": "d1f272ad-5b79-49c1-d9e7-28db7aae0fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Top', 'normaltext'), ('highlight', 'normaltext'), ('Photo', 'normaltext'), ('by', 'normaltext'), ('Engin', 'normaltext'), ('Akyurt', 'normaltext'), ('from', 'normaltext'), ('Pexels', 'normaltext'), ('Facing', 'normaltext'), ('Three', 'normaltext'), ('Fundamental', 'normaltext'), ('Fears', 'normaltext'), ('About', 'normaltext'), ('the', 'normaltext'), ('Coronavirus', 'normaltext'), ('A', 'normaltext'), ('primer', 'normaltext'), ('on', 'normaltext'), ('the', 'normaltext'), ('most', 'normaltext'), ('essential', 'normaltext'), (',', 'normaltext'), ('but', 'normaltext'), ('often', 'normaltext'), ('misunderstood', 'normaltext'), (',', 'normaltext'), ('aspects', 'normaltext'), ('of', 'normaltext'), ('Covid-19', 'normaltext'), ('Bo', 'normaltext'), ('Stapler', 'normaltext'), (',', 'normaltext'), ('MD', 'normaltext'), ('·', 'normaltext'), ('Follow', 'normaltext'), ('Published', 'normaltext'), ('in', 'normaltext'), ('Microbial', 'normaltext'), ('Instincts', 'normaltext'), ('·', 'normaltext'), ('5', 'normaltext'), ('min', 'normaltext'), ('read', 'normaltext'), ('·', 'normaltext'), ('Jun', 'normaltext'), ('29', 'normaltext'), (',', 'normaltext'), ('2020', 'normaltext'), ('461', 'normaltext'), ('4', 'normaltext'), ('A', 'normaltext'), ('mid', 'normaltext'), ('the', 'normaltext'), ('daily', 'normaltext'), ('barrage', 'normaltext'), ('of', 'normaltext'), ('new', 'normaltext'), ('data', 'normaltext'), ('on', 'normaltext'), ('Covid-19', 'normaltext'), (',', 'normaltext'), ('sometimes', 'normaltext'), ('the', 'normaltext'), ('most', 'normaltext'), ('basic', 'normaltext'), ('principles', 'normaltext'), ('of', 'normaltext'), ('the', 'normaltext'), ('disease', 'normaltext'), ('manage', 'normaltext'), ('to', 'normaltext'), ('evade', 'normaltext'), ('the', 'normaltext'), ('headlines', 'normaltext'), ('.', 'normaltext'), ('I', 'normaltext'), ('recognize', 'normaltext'), ('this', 'normaltext'), ('by', 'normaltext'), ('the', 'normaltext'), ('types', 'normaltext'), ('of', 'normaltext'), ('questions', 'normaltext'), ('I', 'normaltext'), ('hear', 'normaltext'), ('on', 'normaltext'), ('a', 'normaltext'), ('regular', 'normaltext'), ('basis', 'normaltext'), ('from', 'normaltext'), ('friends', 'normaltext'), (',', 'normaltext'), ('family', 'normaltext'), (',', 'normaltext'), ('and', 'normaltext'), ('even', 'normaltext'), ('my', 'normaltext'), ('co-workers', 'normaltext'), ('at', 'normaltext'), ('the', 'normaltext'), ('hospital', 'normaltext'), ('.', 'normaltext'), ('Let', 'normaltext'), ('’', 'normaltext'), ('s', 'normaltext'), ('address', 'normaltext'), ('three', 'normaltext'), ('of', 'normaltext'), ('the', 'normaltext'), ('most', 'normaltext'), ('common', 'normaltext'), ('concerns', 'normaltext'), ('.', 'normaltext'), ('1', 'normaltext'), ('.', 'normaltext'), ('Since', 'normaltext'), ('immunity', 'normaltext'), ('to', 'normaltext'), ('the', 'normaltext'), ('novel', 'normaltext'), ('coronavirus', 'normaltext'), ('may', 'normaltext'), ('not', 'normaltext'), ('last', 'normaltext'), ('long', 'normaltext'), (',', 'normaltext'), ('doesn', 'normaltext'), ('’', 'normaltext'), ('t', 'normaltext'), ('the', 'normaltext'), ('virus', 'normaltext'), ('need', 'normaltext'), ('to', 'normaltext'), ('be', 'normaltext'), ('eradicated', 'normaltext'), ('in', 'normaltext'), ('order', 'normaltext'), ('for', 'normaltext'), ('the', 'normaltext'), ('pandemic', 'normaltext'), ('to', 'normaltext'), ('end', 'normaltext'), ('?', 'normaltext'), ('Humans', 'normaltext'), ('generally', 'normaltext'), ('become', 'normaltext'), ('immune', 'normaltext'), ('to', 'normaltext'), ('a', 'normaltext'), ('pathogen', 'normaltext'), ('after', 'normaltext'), ('immunization', 'normaltext'), ('or', 'normaltext'), ('recovery', 'normaltext'), ('from', 'normaltext'), ('an', 'normaltext'), ('infection', 'normaltext'), ('.', 'normaltext'), ('Depending', 'normaltext'), ('on', 'normaltext'), ('the', 'normaltext'), ('particular', 'normaltext'), ('disease', 'normaltext'), ('or', 'normaltext'), ('vaccine', 'normaltext'), ('as', 'normaltext'), ('well', 'normaltext'), ('as', 'normaltext'), ('host', 'normaltext'), ('characteristics', 'normaltext'), ('such', 'normaltext'), ('as', 'normaltext'), ('the', 'normaltext'), ('age', 'normaltext'), ('and', 'normaltext'), ('health', 'normaltext'), ('of', 'normaltext'), ('the', 'normaltext'), ('individual', 'normaltext'), (',', 'normaltext'), ('immunity', 'normaltext'), ('can', 'normaltext'), ('last', 'normaltext'), ('a', 'normaltext'), ('lifetime', 'normaltext'), ('or', 'normaltext'), ('may', 'normaltext'), ('be', 'normaltext'), ('short-lived', 'normaltext'), ('.', 'normaltext'), ('Moreover', 'normaltext'), (',', 'normaltext'), ('immunity', 'normaltext'), ('is', 'normaltext'), ('not', 'normaltext'), ('an', 'normaltext'), ('‘', 'normaltext'), ('either/or', 'normaltext'), ('’', 'normaltext'), ('process', 'normaltext'), ('.', 'normaltext'), ('Our', 'normaltext'), ('immunity', 'normaltext'), ('to', 'normaltext'), ('different', 'normaltext'), ('pathogens', 'normaltext'), ('doesn', 'normaltext'), ('’', 'normaltext'), ('t', 'normaltext'), ('just', 'normaltext'), ('suddenly', 'normaltext'), ('switch', 'normaltext'), ('off', 'normaltext'), ('.', 'normaltext'), ('Instead', 'normaltext'), (',', 'normaltext'), ('it', 'normaltext'), ('wanes', 'normaltext'), ('over', 'normaltext'), ('time', 'normaltext'), ('.', 'normaltext'), ('It', 'normaltext'), ('is', 'normaltext'), ('true', 'normaltext'), ('that', 'normaltext'), ('a', 'normaltext'), ('person', 'normaltext'), ('’', 'normaltext'), ('s', 'normaltext'), ('immunity', 'normaltext'), ('to', 'normaltext'), ('the', 'normaltext'), ('novel', 'normaltext'), ('coronavirus', 'normaltext'), (',', 'normaltext'), ('SARS-CoV-2', 'normaltext'), (',', 'normaltext'), ('probably', 'normaltext'), ('only', 'normaltext'), ('lasts', 'normaltext'), ('a', 'normaltext'), ('few', 'normaltext'), ('months', 'normaltext'), ('to', 'normaltext'), ('a', 'normaltext'), ('few', 'normaltext'), ('years', 'normaltext'), ('.', 'normaltext'), ('It', 'normaltext'), ('is', 'normaltext'), ('also', 'normaltext'), ('true', 'normaltext'), ('that', 'normaltext'), ('outbreaks', 'normaltext'), ('of', 'normaltext'), ('two', 'normaltext'), ('other', 'normaltext'), ('coronaviruses', 'normaltext'), ('earlier', 'normaltext'), ('this', 'normaltext'), ('century', 'normaltext'), (',', 'normaltext'), ('SARS', 'normaltext'), ('and', 'normaltext'), ('MERS', 'normaltext'), (',', 'normaltext'), ('were', 'normaltext'), ('indeed', 'normaltext'), ('contained', 'normaltext'), (',', 'normaltext'), ('although', 'normaltext'), ('not', 'normaltext'), ('technically', 'normaltext'), ('eradicated', 'normaltext'), ('.', 'normaltext'), ('However', 'normaltext'), (',', 'normaltext'), ('the', 'normaltext'), ('other', 'normaltext'), ('four', 'normaltext'), ('known', 'normaltext'), ('human', 'normaltext'), ('coronaviruses', 'normaltext'), (',', 'normaltext'), ('HCoV-229E', 'normaltext'), (',', 'normaltext'), ('-NL63', 'normaltext'), (',', 'normaltext'), ('-OC43', 'normaltext'), (',', 'normaltext'), ('and', 'normaltext'), ('-HKU1', 'normaltext'), (',', 'normaltext'), ('are', 'normaltext'), ('considered', 'normaltext'), ('endemic', 'normaltext'), ('.', 'normaltext'), ('These', 'normaltext'), ('viruses', 'normaltext'), (',', 'normaltext'), ('which', 'normaltext'), ('have', 'normaltext'), ('been', 'normaltext'), ('around', 'normaltext'), ('longer', 'normaltext'), ('than', 'normaltext'), ('the', 'normaltext'), ('other', 'normaltext'), ('three', 'normaltext'), (',', 'normaltext'), ('are', 'normaltext'), ('continuously', 'normaltext'), ('circulating', 'normaltext'), ('through', 'normaltext'), ('the', 'normaltext'), ('population', 'normaltext'), ('and', 'normaltext'), ('typically', 'normaltext'), ('cause', 'normaltext'), ('no', 'normaltext'), ('more', 'normaltext'), ('symptoms', 'normaltext'), ('than', 'normaltext'), ('the', 'normaltext'), ('common', 'normaltext'), ('cold', 'normaltext'), ('.', 'normaltext'), ('Interestingly', 'normaltext'), (',', 'normaltext'), ('there', 'normaltext'), ('is', 'normaltext'), ('historical', 'normaltext'), ('evidence', 'normaltext'), ('that', 'normaltext'), ('the', 'normaltext'), ('four', 'normaltext'), ('endemic', 'normaltext'), ('coronaviruses', 'normaltext'), ('were', 'normaltext'), ('likely', 'normaltext'), ('the', 'normaltext'), ('cause', 'normaltext'), ('of', 'normaltext'), ('pandemics', 'normaltext'), (',', 'normaltext'), ('or', 'normaltext'), ('at', 'normaltext'), ('least', 'normaltext'), ('epidemics', 'normaltext'), (',', 'normaltext'), ('in', 'normaltext'), ('the', 'normaltext'), ('past', 'normaltext'), ('.', 'normaltext'), ('Of', 'normaltext'), ('course', 'normaltext'), (',', 'normaltext'), ('this', 'normaltext'), ('would', 'normaltext'), ('have', 'normaltext'), ('been', 'normaltext'), ('long', 'normaltext'), ('enough', 'normaltext'), ('ago', 'normaltext'), ('that', 'normaltext'), ('people', 'normaltext'), ('didn', 'normaltext'), ('’', 'normaltext'), ('t', 'normaltext'), ('know', 'normaltext'), ('what', 'normaltext'), ('a', 'normaltext'), ('coronavirus', 'normaltext'), ('was', 'normaltext'), (',', 'normaltext'), ('but', 'normaltext'), ('humankind', 'normaltext'), ('managed', 'normaltext'), ('to', 'normaltext'), ('recover', 'normaltext'), ('anyway', 'normaltext'), ('.', 'normaltext'), ('These', 'normaltext'), ('older', 'normaltext'), ('coronaviruses', 'normaltext'), ('now', 'normaltext'), ('permeate', 'normaltext'), ('among', 'normaltext'), ('the', 'normaltext'), ('population', 'normaltext'), ('.', 'normaltext'), ('People', 'normaltext'), ('often', 'normaltext'), ('become', 'normaltext'), ('exposed', 'normaltext'), ('at', 'normaltext'), ('a', 'normaltext'), ('young', 'normaltext'), ('age', 'normaltext'), (',', 'normaltext'), ('and', 'normaltext'), (',', 'normaltext'), ('as', 'normaltext'), ('with', 'normaltext'), ('SARS-CoV-2', 'normaltext'), (',', 'normaltext'), ('the', 'normaltext'), ('vast', 'normaltext'), ('majority', 'normaltext'), ('of', 'normaltext'), ('children', 'normaltext'), ('have', 'normaltext'), ('no', 'normaltext'), ('symptoms', 'normaltext'), ('or', 'normaltext'), ('a', 'normaltext'), ('minor', 'normaltext'), ('respiratory', 'normaltext'), ('infection', 'normaltext'), ('.', 'normaltext'), ('Even', 'normaltext'), ('though', 'normaltext'), ('immunity', 'normaltext'), ('to', 'normaltext'), ('these', 'normaltext'), ('viruses', 'normaltext'), ('diminishes', 'normaltext'), ('over', 'normaltext'), ('time', 'normaltext'), (',', 'normaltext'), ('because', 'normaltext'), ('they', 'normaltext'), ('are', 'normaltext'), ('endemic', 'normaltext'), ('and', 'normaltext'), ('continuously', 'normaltext'), ('circulate', 'normaltext'), ('through', 'normaltext'), ('the', 'normaltext'), ('population', 'normaltext'), (',', 'normaltext'), ('every', 'normaltext'), ('few', 'normaltext'), ('years', 'normaltext'), ('our', 'normaltext'), ('immune', 'normaltext'), ('systems', 'normaltext'), ('are', 'normaltext'), ('again', 'normaltext'), ('exposed', 'normaltext'), ('and', 'normaltext'), ('receive', 'normaltext'), ('a', 'normaltext'), ('refresher', 'normaltext'), ('course', 'normaltext'), ('on', 'normaltext'), ('how', 'normaltext'), ('to', 'normaltext'), ('kill', 'normaltext'), ('the', 'normaltext'), ('virus', 'normaltext'), ('.', 'normaltext'), ('With', 'normaltext'), ('possible', 'normaltext'), ('exception', 'normaltext'), ('to', 'normaltext'), ('geographically', 'normaltext'), ('isolated', 'normaltext'), ('locations', 'normaltext'), ('like', 'normaltext'), ('Iceland', 'normaltext'), ('and', 'normaltext'), ('New', 'normaltext'), ('Zealand', 'normaltext'), (',', 'normaltext'), ('SARS-CoV-2', 'normaltext'), ('is', 'normaltext'), ('on', 'normaltext'), ('a', 'normaltext'), ('path', 'normaltext'), ('to', 'normaltext'), ('becoming', 'normaltext'), ('endemic', 'normaltext'), ('like', 'normaltext'), ('its', 'normaltext'), ('older', 'normaltext'), ('endemic', 'normaltext'), ('coronavirus', 'normaltext'), ('siblings', 'normaltext'), ('.', 'normaltext'), ('In', 'normaltext'), ('fact', 'normaltext'), (',', 'normaltext'), ('since', 'normaltext'), ('March', 'normaltext'), (',', 'normaltext'), ('for', 'normaltext'), ('most', 'normaltext'), ('regions', 'normaltext'), ('containment', 'normaltext'), ('has', 'normaltext'), ('no', 'normaltext'), ('longer', 'normaltext'), ('been', 'normaltext'), ('a', 'normaltext'), ('strategy', 'normaltext'), ('for', 'normaltext'), ('managing', 'normaltext'), ('SARS-CoV-2', 'normaltext'), ('as', 'normaltext'), ('it', 'normaltext'), ('was', 'normaltext')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### seperate tokenization ###\n",
        "\n",
        "\n",
        "def chunker(seq, size):\n",
        "    \"\"\"Divide a sequence into chunks of the given size.\"\"\"\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "\n",
        "# Split your tagged_data into a training set and a test set\n",
        "split_index = int(len(tagged_data) * 0.8)\n",
        "train_data = tagged_data[:split_index]\n",
        "test_data = tagged_data[split_index:]\n",
        "\n",
        "# Get the unique words and labels in the training data\n",
        "words_train = list(set(token for seq in train_data for token, label in seq))\n",
        "tags_train = list(set(label for seq in train_data for token, label in seq))\n",
        "\n",
        "# Tokenize words\n",
        "word_tokenizer_train = Tokenizer(filters='', lower=False, oov_token='OOV')\n",
        "word_tokenizer_train.fit_on_texts(words_train)\n",
        "\n",
        "# Tokenize words for test data\n",
        "word_tokenizer_test = Tokenizer(filters='', lower=False, oov_token='OOV')\n",
        "word_tokenizer_test.fit_on_texts(words_train)  # Note: Fit on words from training data only!\n",
        "\n",
        "# Tokenize labels\n",
        "label_tokenizer = Tokenizer(filters='', lower=False)\n",
        "label_tokenizer.word_index = {'normaltext': 1, 'highlight': 2}\n",
        "\n",
        "def process_chunk(chunk, max_sequence_length, word_tokenizer):\n",
        "    X_chunk = []\n",
        "    y_chunk = []\n",
        "\n",
        "    for seq in chunk:\n",
        "        document_words = []\n",
        "        document_labels = []\n",
        "\n",
        "        for word, label in seq:\n",
        "            # Transform each word to a numeric ID\n",
        "            encoded_word = word_tokenizer.word_index.get(word, word_tokenizer.word_index['OOV'])\n",
        "            document_words.append(encoded_word)\n",
        "\n",
        "            # Encode the label for the word\n",
        "            encoded_label = label_tokenizer.word_index[label]\n",
        "            document_labels.append(encoded_label)\n",
        "\n",
        "        # Pad the document_words and document_labels to max_sequence_length\n",
        "        document_words = pad_sequences([document_words], maxlen=max_sequence_length, padding='post')[0]\n",
        "        document_labels = pad_sequences([document_labels], maxlen=max_sequence_length, padding='post')[0]\n",
        "\n",
        "        X_chunk.append(document_words)\n",
        "        y_chunk.append(document_labels)\n",
        "\n",
        "    return X_chunk, y_chunk\n",
        "\n",
        "# Define max_sequence_length\n",
        "max_sequence_length = 512\n",
        "\n",
        "# Process chunks for train and test data\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for chunk in chunker(train_data, 1000):  # Choose chunk size based on your memory capacity\n",
        "    X_chunk, y_chunk = process_chunk(chunk, max_sequence_length, word_tokenizer_train)\n",
        "    X_train.extend(X_chunk)\n",
        "    y_train.extend(y_chunk)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for chunk in chunker(test_data, 1000):  # Choose chunk size based on your memory capacity\n",
        "    X_chunk, y_chunk = process_chunk(chunk, max_sequence_length, word_tokenizer_test)\n",
        "    X_test.extend(X_chunk)\n",
        "    y_test.extend(y_chunk)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n"
      ],
      "metadata": {
        "id": "7AanSaP7N4aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T3IZCxEOHP8",
        "outputId": "6c1455b4-6a2e-44cd-a3e9-7a67d812250c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9240, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/thesis_files/\""
      ],
      "metadata": {
        "id": "vELBLt4R_GgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Save X_train data\n",
        "with h5py.File(f'{folder_path}X_train_data_wordlabel_26.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"X_train_data\",  data=np.array(X_train))\n",
        "\n",
        "# Save y_train data\n",
        "with h5py.File(f'{folder_path}y_train_data_wordlabel_26.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"y_train_data\",  data=np.array(y_train))\n",
        "\n",
        "# Save X_test data\n",
        "with h5py.File(f'{folder_path}X_test_data_wordlabel_26.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"X_test_data\",  data=np.array(X_test))\n",
        "\n",
        "# Save y_test data\n",
        "with h5py.File(f'{folder_path}y_test_data_wordlabel_26.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"y_test_data\",  data=np.array(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "yFsg17hFP2yq",
        "outputId": "98b2a86a-db19-4814-b2a1-0aca2361d7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-31e3e2573efd>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Save X_train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{folder_path}X_train_data_wordlabel_26.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train_data\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Save y_train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load X_train data\n",
        "with h5py.File(f'{folder_path}X_train_data_wordlabel_26.h5', 'r') as hf:\n",
        "    X_train = np.array(hf['X_train_data'])\n",
        "\n",
        "# Load y_train data\n",
        "with h5py.File(f'{folder_path}y_train_data_wordlabel_26.h5', 'r') as hf:\n",
        "    y_train = np.array(hf['y_train_data'])\n",
        "\n",
        "# Load X_test data\n",
        "with h5py.File(f'{folder_path}X_test_data_wordlabel_26.h5', 'r') as hf:\n",
        "    X_test = np.array(hf['X_test_data'])\n",
        "\n",
        "# Load y_test data\n",
        "with h5py.File(f'{folder_path}y_test_data_wordlabel_26.h5', 'r') as hf:\n",
        "    y_test = np.array(hf['y_test_data'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "WCPDNv7WP_QD",
        "outputId": "c9f6b696-0e98-4731-aeb6-fb57e7b3a4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-843dc86a4057>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load X_train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{folder_path}X_train_data_wordlabel_26.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load y_train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'X_train_data' doesn't exist)\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IweM_j0DQJgh",
        "outputId": "55aad7e7-bedf-442c-84ef-3e9fd1af3418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79292, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### save the sentence-labelled data ###\n",
        "\n",
        "import h5py\n",
        "\n",
        "# Specify the path to the folder where you want to save the files\n",
        "\n",
        "\n",
        "# Save X data\n",
        "with h5py.File(f'{folder_path}X_data_wordlabel_20.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"X_data\",  data=np.array(X))\n",
        "\n",
        "# Save y data\n",
        "with h5py.File(f'{folder_path}y_data_wordlabel_20.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"y_data\",  data=np.array(y))\n"
      ],
      "metadata": {
        "id": "6nHQxrMI-2yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load X data\n",
        "import h5py\n",
        "\n",
        "with h5py.File(f'{folder_path}X_data_wordlabel_20.h5', 'r') as hf:\n",
        "    X = hf['X_data'][:]\n",
        "\n",
        "# Load y data\n",
        "with h5py.File(f'{folder_path}y_data_wordlabel_20.h5', 'r') as hf:\n",
        "    y = hf['y_data'][:]\n"
      ],
      "metadata": {
        "id": "psF_xXBW_LKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "0wDB9NODFZbC",
        "outputId": "fa2ef5ef-dc36-42e9-9ad5-31b70ef88874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-94cb9ce28558>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_tokenizer_train.word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "036Y_GcDOjtz",
        "outputId": "a6066337-d46e-4f46-ba0f-c71a5795f422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "902788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_word_index = 902788 # 104385 #23031 # 26892 # 974442 #len(words)\n",
        "print(len_word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pWbGkPaCJAS",
        "outputId": "527c2be2-800d-43f7-9536-c1516cd4f7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "902788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53q8yEXFwV9q",
        "outputId": "abca73b5-b569-4e6a-c0e5-c279944856a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26892"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_tag_index = 3 # len(tags) +1\n",
        "print(len_tag_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcFQp8KPCNva",
        "outputId": "5c45dc86-8464-4a68-e2cd-15fce2404a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tagged_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw3g2bJXv36J",
        "outputId": "3aedbc15-28d1-420f-d7dc-f977a9b36e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Flatten the nested list:\n",
        "flattened = np.array(X_train).flatten()\n",
        "\n",
        "max_index = np.max(flattened)\n",
        "\n",
        "print(\"Maximum index in training data: \", max_index)\n",
        "print(\"Length of word index: \", len_word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsCgPnA5m2kq",
        "outputId": "47f46d2e-d10c-4c9a-886d-4cede8df0a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum index in training data:  902789\n",
            "Length of word index:  902788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the structure of the model\n",
        "document_input = Input(shape=(None,), dtype='int32')  # shape = (num_documents, num_words)\n",
        "embedded_sequences = Embedding(input_dim=len_word_index +2, output_dim=50)(document_input)  # embedding at word level ## watch out for +1 or +2 here\n",
        "masked_sequences = Masking(mask_value=0)(embedded_sequences)  # masking at word level\n",
        "word_lstm = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(masked_sequences)  # LSTM at word level, returning sequences\n",
        "word_output = Dense(len_tag_index, activation='softmax')(word_lstm)  # word-level output\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[document_input], outputs=[word_output])\n"
      ],
      "metadata": {
        "id": "uNwTGG3MCGcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute sample weights\n",
        "unique, counts = np.unique([label for document in y_train for label in document], return_counts=True)\n",
        "weights = len(y_train) / (len(unique) * counts)\n",
        "\n",
        "# Adjust the weight for the minority class (highlight class)\n",
        "weights = weights * len(unique)  # normalization to make minority class weight >= 1\n",
        "weights_dict = {class_id: weight for class_id, weight in zip(unique, weights)}\n",
        "\n",
        "# Increase weight for the highlight class (class 2)\n",
        "weights_dict[2] = weights_dict[2] * 1  # You can adjust this value based on your requirements\n",
        "\n",
        "# Set the weight of the padding class to 0\n",
        "weights_dict[0] = 0\n",
        "\n",
        "# Create a sample weights matrix and assign a weight to each word in each document\n",
        "sample_weights = []\n",
        "\n",
        "for document_labels in y_train:\n",
        "    document_weights = [weights_dict[label] for label in document_labels.tolist()]\n",
        "    sample_weights.append(document_weights)\n",
        "\n",
        "\n",
        "# Make sure to convert sample_weights to a numpy array\n",
        "sample_weights = np.array(sample_weights)\n"
      ],
      "metadata": {
        "id": "dz-vyB8Uy70N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weights_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LCZSmTuDxhL",
        "outputId": "695fa7a2-fa19-435a-87e3-dd9c8e078b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0, 1: 0.0020669436685382694, 2: 0.18310717815608862}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Convert y and sample_weights to numpy arrays and expand their last dimensions\n",
        "#y = np.array(y)\n",
        "y_train = np.expand_dims(y_train, -1)  # model expects 3D array for y\n",
        "y_test = np.expand_dims(y_test, -1) # check if this is needed\n",
        "\n",
        "#sample_weights = np.array(sample_weights)\n",
        "sample_weights = np.expand_dims(sample_weights, -1)  # model expects 3D array for sample_weights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BSYZwYsQC8dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(sample_weights.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "enIISVMbxXs7",
        "outputId": "4916ef05-b11b-4f18-d4df-7732f6ba0fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-ec5f27a14c85>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    sample_weight=sample_weights,\n",
        "                    epochs=3, batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRsjlKVxETQa",
        "outputId": "d0a91da4-fc6f-4736-9a4a-541c357091bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "496/496 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.7056"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r496/496 [==============================] - 457s 911ms/step - loss: 0.0024 - accuracy: 0.7056 - val_loss: 0.0032 - val_accuracy: 0.0942\n",
            "Epoch 2/3\n",
            "496/496 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.7313"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r496/496 [==============================] - 453s 912ms/step - loss: 0.0019 - accuracy: 0.7313 - val_loss: 0.0027 - val_accuracy: 0.4247\n",
            "Epoch 3/3\n",
            "496/496 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.7858"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r496/496 [==============================] - 452s 911ms/step - loss: 0.0016 - accuracy: 0.7858 - val_loss: 0.0029 - val_accuracy: 0.4970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_name = 'model_word_27_1'\n",
        "full_path = folder_path + model_name\n",
        "model.save(full_path)"
      ],
      "metadata": {
        "id": "Vqd-32ArGVFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Since our outputs are softmax probabilities, we need to choose the class with the highest probability\n",
        "y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "# Flatten y_test and y_pred_classes to be compatible with classification_report\n",
        "y_test_flatten = [label for doc in y_test for label in doc]\n",
        "y_pred_classes_flatten = [pred for doc in y_pred_classes for pred in doc]\n",
        "\n",
        "# We also need to remove padding (class 0) predictions for the classification report\n",
        "y_test_no_padding = []\n",
        "y_pred_no_padding = []\n",
        "\n",
        "for true, pred in zip(y_test_flatten, y_pred_classes_flatten):\n",
        "    if true != 0:  # Exclude padding class\n",
        "        y_test_no_padding.append(true)\n",
        "        y_pred_no_padding.append(pred)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test_no_padding, y_pred_no_padding, target_names=['normaltext', 'highlight']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRGjN85xGQqL",
        "outputId": "f6668c4d-1921-41e6-f439-2c881746d26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "620/620 [==============================] - 62s 100ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  normaltext       0.99      0.42      0.59   9267984\n",
            "   highlight       0.02      0.74      0.04    155322\n",
            "\n",
            "    accuracy                           0.43   9423306\n",
            "   macro avg       0.51      0.58      0.32   9423306\n",
            "weighted avg       0.97      0.43      0.58   9423306\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the maximum index in the X_test\n",
        "max_index = np.max(X_test)\n",
        "print(f'Max index in X_test: {max_index}')\n",
        "\n",
        "# check the size of the word index\n",
        "print(f'Size of word index: {len_word_index}')\n",
        "\n",
        "# if max_index >= len_word_index, then you have a problem\n",
        "if max_index >= len_word_index:\n",
        "    print(\"Problem detected: some word indices in X_test exceed the size of the word index.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMxg1ERstWAL",
        "outputId": "81546946-87c7-4e20-af07-f4d8c144bb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max index in X_test: 974443\n",
            "Size of word index: 974442\n",
            "Problem detected: some word indices in X_test exceed the size of the word index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the document and word where the error occurred\n",
        "doc_index, word_index = 15, 145\n",
        "doc = X_test[doc_index]\n",
        "word = doc[word_index]\n",
        "print(f'Problematic word index: {word}')\n",
        "\n",
        "# see if this word is in the word_index\n",
        "for key, value in word_index.items():\n",
        "    if value == word:\n",
        "        print(f'Word: {key}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "brQsAV6utblk",
        "outputId": "d1e7308e-8bc0-40fa-c5d0-d5c042df275a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problematic word index: 967415\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d87ddd3fd781>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# see if this word is in the word_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Word: {key}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the document and word where the error occurred\n",
        "doc_index, index = 15, 145\n",
        "doc = X_test[doc_index]\n",
        "word = doc[index]\n",
        "print(f'Problematic word index: {word}')\n",
        "\n",
        "# see if this word is in the word_index\n",
        "for key, value in word_index.items():\n",
        "    if value == word:\n",
        "        print(f'Word: {key}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "J0GDPF-vtsOy",
        "outputId": "0e851688-ce16-4ae9-ae8e-7ba28f887a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problematic word index: 967415\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-fbdd357bd03e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# see if this word is in the word_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Word: {key}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'items'"
          ]
        }
      ]
    }
  ]
}